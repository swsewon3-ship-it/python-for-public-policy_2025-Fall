{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swsewon3-ship-it/python-for-public-policy_2025-Fall/blob/main/Session1_APIs_WebScraping_Workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "508188a6",
      "metadata": {
        "id": "508188a6"
      },
      "source": [
        "# Session 1 Workbook — Data Collection with APIs & Web Scraping\n",
        "### Text Analysis in Python for Public Policy / International Affairs\n",
        "\n",
        "\n",
        "**Structure (2.5h live coding → 30m break → 60m student work)**\n",
        "1. Token‑less API warm‑up (JSON & endpoints)\n",
        "2. NewsAPI.org — query 'New York' → JSON → pandas DataFrame → quick clean\n",
        "3. Web scraping with BeautifulSoup using the URL column\n",
        "4. Ethics & troubleshooting notes\n",
        "\n",
        "> Tip: Run cells top‑to‑bottom. If you hit network/rate‑limit issues, use the *Offline Fallback* cells provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cba3dde",
      "metadata": {
        "id": "5cba3dde"
      },
      "source": [
        "## Learning Objectives\n",
        "By the end of this session, you will be able to:\n",
        "- Explain what an API is (base URL, endpoints, query parameters) and read JSON.\n",
        "- Make a request to a public API and parse JSON into Python objects.\n",
        "- Convert nested JSON into a tidy pandas `DataFrame` and do light cleaning.\n",
        "- Use article URLs from API results to scrape page text with BeautifulSoup.\n",
        "- Follow basic ethical guidelines for scraping (robots.txt, rate limiting, attribution).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee60d4a",
      "metadata": {
        "id": "cee60d4a"
      },
      "source": [
        "## 0) Environment Setup (Colab‑friendly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc44b7a",
      "metadata": {
        "id": "bcc44b7a"
      },
      "outputs": [],
      "source": [
        "# If you're in Colab, uncomment the next lines to install packages:\n",
        "# !pip install requests pandas beautifulsoup4 lxml python-dotenv\n",
        "\n",
        "# Imports used throughout the workbook\n",
        "import requests            # for HTTP requests to APIs / web pages\n",
        "import json                # for pretty-printing JSON\n",
        "import pandas as pd        # for DataFrame work\n",
        "from bs4 import BeautifulSoup  # for HTML parsing\n",
        "import time                # for polite sleeping / rate-limiting\n",
        "import os                  # for reading environment variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb929199",
      "metadata": {
        "id": "bb929199"
      },
      "source": [
        "## 1) Warm‑up: Token‑less API (Cat Facts API)\n",
        "**Concepts:** base URL, endpoint, JSON, key–value pairs.\n",
        "\n",
        "We’ll hit a simple, no‑auth endpoint to focus on the response shape.\n",
        "API docs: `https://catfact.ninja/fact` (returns one random cat fact)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7acea85b",
      "metadata": {
        "id": "7acea85b"
      },
      "outputs": [],
      "source": [
        "# Define the base URL (the main address of the API).\n",
        "base_url = \"https://catfact.ninja\"\n",
        "# Define the endpoint (the specific resource we want under the base URL).\n",
        "endpoint = \"/fact\"\n",
        "# Combine base URL and endpoint into a full URL.\n",
        "url = f\"{base_url}{endpoint}\"\n",
        "\n",
        "# Send a GET request to the server to retrieve data.\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check the HTTP status code (200 means OK/success).\n",
        "print(\"Status code:\", response.status_code)\n",
        "\n",
        "# Convert the response body from JSON text into a Python dict.\n",
        "data = response.json()\n",
        "\n",
        "# Pretty-print the JSON so we can see its structure (keys and values).\n",
        "print(\"Raw JSON:\")\n",
        "print(json.dumps(data, indent=2))\n",
        "\n",
        "# Access a value by key from the JSON (dictionary).\n",
        "print(\"Just the fact value:\")\n",
        "print(data[\"fact\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "286173c2",
      "metadata": {
        "id": "286173c2"
      },
      "source": [
        "### Offline Fallback (if the API is down or blocked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d84d2f",
      "metadata": {
        "id": "02d84d2f"
      },
      "outputs": [],
      "source": [
        "# This cell simulates the same JSON structure returned by the API.\n",
        "# Use this if you're offline or the API rate-limits during class.\n",
        "offline_json = {\n",
        "    \"fact\": \"Cats can rotate their ears 180 degrees.\",\n",
        "    \"length\": 41\n",
        "}\n",
        "print(json.dumps(offline_json, indent=2))\n",
        "print(\"Access 'fact':\", offline_json[\"fact\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a82d7a8c",
      "metadata": {
        "id": "a82d7a8c"
      },
      "source": [
        "## 2) Real‑world API: NewsAPI.org — `everything` endpoint\n",
        "We’ll search for the term **“New York”**, retrieve results in JSON, then convert to a pandas `DataFrame`.\n",
        "\n",
        "> **Setup:** You need a free API key from https://newsapi.org/  \n",
        "> **Security tip:** Store your key as an environment variable (e.g., `NEWSAPI_KEY`) or use `python-dotenv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f782c181",
      "metadata": {
        "id": "f782c181"
      },
      "outputs": [],
      "source": [
        "# Store your API KEY in a variab;e\n",
        "NEWSAPI_KEY = '123456789'\n",
        "\n",
        "# Define the endpoint and query parameters.\n",
        "news_url = \"https://newsapi.org/v2/everything\"\n",
        "params = {\n",
        "    \"q\": \"New York\",   # search query\n",
        "    \"language\": \"en\",  # restrict to English\n",
        "    \"pageSize\": 25,    # number of results per page (max 100)\n",
        "    \"sortBy\": \"relevancy\",  # or 'publishedAt' for recency\n",
        "    \"apiKey\": NEWSAPI_KEY   # your API key\n",
        "}\n",
        "\n",
        "# Make the request with parameters to the NewsAPI endpoint.\n",
        "news_resp = requests.get(news_url, params=params)\n",
        "\n",
        "# Inspect status code to ensure the request worked.\n",
        "print(\"Status code:\", news_resp.status_code)\n",
        "\n",
        "# Convert to Python objects (dict) from JSON.\n",
        "news_data = news_resp.json()\n",
        "\n",
        "# Sanity check: print the top-level keys.\n",
        "print(\"Top-level keys:\", list(news_data.keys()))\n",
        "\n",
        "# Inspect the first article's keys to understand the structure.\n",
        "if news_data.get(\"articles\"):\n",
        "    print(\"Article keys:\", list(news_data[\"articles\"][0].keys()))\n",
        "else:\n",
        "    print(\"No articles returned. Check your API key or params.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fc1990",
      "metadata": {
        "id": "b2fc1990"
      },
      "source": [
        "### Convert JSON → pandas DataFrame & Quick Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3031479",
      "metadata": {
        "id": "a3031479"
      },
      "outputs": [],
      "source": [
        "# Convert the list of articles (list of dicts) into a DataFrame.\n",
        "articles = news_data.get(\"articles\", [])\n",
        "df = pd.DataFrame(articles)\n",
        "\n",
        "# Show the first few rows to confirm shape and columns.\n",
        "print(\"DataFrame shape:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef494dce",
      "metadata": {
        "id": "ef494dce"
      },
      "outputs": [],
      "source": [
        "# Select a subset of useful columns for our analysis.\n",
        "keep_cols = [\"source\", \"author\", \"title\", \"description\", \"url\", \"publishedAt\"]\n",
        "df = df[keep_cols]\n",
        "\n",
        "# 'source' is a nested dict; flatten it to a simple string (source name).\n",
        "# We create a new column 'source_name' with the inner 'name' field.\n",
        "df[\"source_name\"] = df[\"source\"].apply(lambda d: d.get(\"name\") if isinstance(d, dict) else None)\n",
        "\n",
        "# Drop the original nested 'source' column now that we've extracted the name.\n",
        "df = df.drop(columns=[\"source\"])\n",
        "\n",
        "# Convert 'publishedAt' to a proper datetime type for easier filtering/sorting.\n",
        "df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"], errors=\"coerce\")\n",
        "\n",
        "# Drop rows with missing URLs or titles (these are critical for scraping/analysis).\n",
        "df = df.dropna(subset=[\"url\", \"title\"]).reset_index(drop=True)\n",
        "\n",
        "# Sort by recency to bring the newest items to the top.\n",
        "df = df.sort_values(\"publishedAt\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display a tidy preview\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c56dd3c6",
      "metadata": {
        "id": "c56dd3c6"
      },
      "source": [
        "### Offline Fallback for NewsAPI (sample payload)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24be0f55",
      "metadata": {
        "id": "24be0f55"
      },
      "outputs": [],
      "source": [
        "# Use a small, hard-coded sample if the API call fails/limits.\n",
        "offline_news = {\n",
        "  \"status\": \"ok\",\n",
        "  \"totalResults\": 2,\n",
        "  \"articles\": [\n",
        "    {\n",
        "      \"source\": {\n",
        "        \"id\": null,\n",
        "        \"name\": \"Example Times\"\n",
        "      },\n",
        "      \"author\": \"Jane Doe\",\n",
        "      \"title\": \"New York expands ferry service for commuters\",\n",
        "      \"description\": \"City officials announce new routes and schedules.\",\n",
        "      \"url\": \"https://www.example.com/ny-ferry\",\n",
        "      \"publishedAt\": \"2025-10-20T10:00:00Z\"\n",
        "    },\n",
        "    {\n",
        "      \"source\": {\n",
        "        \"id\": null,\n",
        "        \"name\": \"Policy Daily\"\n",
        "      },\n",
        "      \"author\": \"John Smith\",\n",
        "      \"title\": \"Housing advocates push for zoning reform in New York\",\n",
        "      \"description\": \"Debate intensifies over upzoning proposals.\",\n",
        "      \"url\": \"https://www.example.com/ny-zoning\",\n",
        "      \"publishedAt\": \"2025-10-19T09:30:00Z\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "offline_df = pd.DataFrame(offline_news[\"articles\"])\n",
        "offline_df[\"source_name\"] = offline_df[\"source\"].apply(lambda d: d.get(\"name\") if isinstance(d, dict) else None)\n",
        "offline_df = offline_df.drop(columns=[\"source\"])\n",
        "offline_df[\"publishedAt\"] = pd.to_datetime(offline_df[\"publishedAt\"], errors=\"coerce\")\n",
        "offline_df = offline_df.dropna(subset=[\"url\", \"title\"]).sort_values(\"publishedAt\", ascending=False).reset_index(drop=True)\n",
        "offline_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b00548f",
      "metadata": {
        "id": "7b00548f"
      },
      "source": [
        "## 3) Web Scraping with BeautifulSoup (from the URL column)\n",
        "**Goal:** Given an article URL, fetch the web page and extract the textual content (paragraphs).\n",
        "\n",
        "> **Important:** Real news sites often have paywalls or dynamic content loaded by JavaScript. For teaching, start with any URL that returns visible `<p>` text. Otherwise, use the **offline fallback** cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "516119b6",
      "metadata": {
        "id": "516119b6"
      },
      "outputs": [],
      "source": [
        "# Choose a URL to scrape: try the live df first, fallback to offline_df if needed.\n",
        "candidate_df = df if not df.empty else offline_df\n",
        "article_url = candidate_df.loc[0, \"url\"]\n",
        "print(\"Scraping URL:\", article_url)\n",
        "\n",
        "# Send a GET request to retrieve the raw HTML of the page.\n",
        "page_resp = requests.get(article_url, timeout=15)\n",
        "\n",
        "# Create a BeautifulSoup object to parse the HTML document.\n",
        "soup = BeautifulSoup(page_resp.text, \"html.parser\")\n",
        "\n",
        "# Find all paragraph tags <p> and extract text from each.\n",
        "paragraphs = soup.find_all(\"p\")\n",
        "\n",
        "# Use a list comprehension to strip whitespace and only keep non-empty paragraphs.\n",
        "para_text = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
        "\n",
        "# Join paragraphs into a single string for quick inspection (limit output length).\n",
        "full_text = \" \".join(para_text)\n",
        "print(\"First 1000 characters of extracted text:\\n\")\n",
        "print(full_text[:1000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5736ca92",
      "metadata": {
        "id": "5736ca92"
      },
      "source": [
        "### (Optional) More robust scraping: headers + polite delay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0725f4",
      "metadata": {
        "id": "3e0725f4"
      },
      "outputs": [],
      "source": [
        "# Some sites block default Python requests; set a user-agent header to look like a browser.\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "                  \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Example loop to scrape first N article URLs with a polite delay.\n",
        "N = min(3, len(candidate_df))  # limit to 3 for class demo\n",
        "texts = []\n",
        "\n",
        "for i in range(N):\n",
        "    url_i = candidate_df.loc[i, \"url\"]\n",
        "    print(f\"Fetching ({i+1}/{N}):\", url_i)\n",
        "    try:\n",
        "        r = requests.get(url_i, headers=headers, timeout=15)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        paras = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n",
        "        text_i = \" \".join([t for t in paras if t])\n",
        "        texts.append(text_i)\n",
        "        # Be polite and avoid hammering servers.\n",
        "        time.sleep(1.0)\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching:\", e)\n",
        "        texts.append(\"\")\n",
        "\n",
        "# Add scraped text as a new column aligned to the first N rows.\n",
        "candidate_df = candidate_df.copy()\n",
        "candidate_df.loc[:N-1, \"scraped_text\"] = texts\n",
        "candidate_df.head(N)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9601afe2",
      "metadata": {
        "id": "9601afe2"
      },
      "source": [
        "## 4) Save Scraped Text to Google Drive as `.txt` Files (Colab)\n",
        "This section lets you export each row's `scraped_text` into a separate `.txt` file on Google Drive.\n",
        "\n",
        "**Workflow**\n",
        "1. Mount Drive (Colab)\n",
        "2. Choose (or create) a destination folder in your Drive\n",
        "3. Iterate through the DataFrame, sanitize filenames, and write `.txt` files\n",
        "\n",
        "> If you're running **locally** (not in Colab), skip the mount cell and set `base_dir` to a local path (e.g., `./exports`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c9b28a",
      "metadata": {
        "id": "a6c9b28a"
      },
      "outputs": [],
      "source": [
        "# --- Colab-only: Mount Google Drive ---\n",
        "# If running in Colab, uncomment the two lines below.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Choose a Drive folder (adjust this path). If running locally, use a local path instead.\n",
        "# Example for Colab:\n",
        "base_dir = \"/content/drive/MyDrive/text-analysis/session1_articles\"\n",
        "# Example local fallback:\n",
        "# base_dir = \"./exports\"\n",
        "\n",
        "import os\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "print(\"Saving .txt files to:\", base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cda80a2",
      "metadata": {
        "id": "9cda80a2"
      },
      "outputs": [],
      "source": [
        "# --- Export each row's scraped_text to a .txt file ---\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def slugify(text, max_len=80):\n",
        "    \"\"\"Create a filesystem-safe slug from any text.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"untitled\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9]+\", \"-\", text)\n",
        "    text = re.sub(r\"-{2,}\", \"-\", text).strip(\"-\")\n",
        "    return text[:max_len] if text else \"untitled\"\n",
        "\n",
        "# Pick the first non-empty DataFrame among common variables created earlier.\n",
        "df_source = None\n",
        "for name in [\"df_topic\", \"candidate_df\", \"df\", \"offline_df\"]:\n",
        "    if name in globals():\n",
        "        _df = globals()[name]\n",
        "        if isinstance(_df, pd.DataFrame) and not _df.empty:\n",
        "            df_source = _df\n",
        "            print(f\"Using DataFrame: {name} with shape {_df.shape}\")\n",
        "            break\n",
        "\n",
        "if df_source is None:\n",
        "    raise ValueError(\"No DataFrame available. Run earlier cells to create df_topic/df/candidate_df/offline_df.\")\n",
        "\n",
        "if \"scraped_text\" not in df_source.columns:\n",
        "    print(\"`scraped_text` column not found. You may need to run the scraping cells first.\")\n",
        "else:\n",
        "    used_names = set()\n",
        "    saved = 0\n",
        "    for i, row in df_source.iterrows():\n",
        "        text = row.get(\"scraped_text\", \"\")\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            continue  # skip empty text rows\n",
        "\n",
        "        # Build a filename using date + title/url slug for disambiguation.\n",
        "        title = row.get(\"title\") if \"title\" in df_source.columns else None\n",
        "        url = row.get(\"url\") if \"url\" in df_source.columns else None\n",
        "\n",
        "        # Try to extract a date for filename\n",
        "        date_str = \"\"\n",
        "        if \"publishedAt\" in df_source.columns and pd.notna(row.get(\"publishedAt\")):\n",
        "            try:\n",
        "                date_str = pd.to_datetime(row[\"publishedAt\"]).strftime(\"%Y%m%d\")\n",
        "            except Exception:\n",
        "                date_str = \"\"\n",
        "\n",
        "        base_name = slugify(title) if title else (slugify(url) if url else f\"article-{i}\")\n",
        "        fname = f\"{date_str + '_' if date_str else ''}{base_name}.txt\"\n",
        "\n",
        "        # Ensure uniqueness\n",
        "        original_fname = fname\n",
        "        k = 2\n",
        "        while fname in used_names or os.path.exists(os.path.join(base_dir, fname)):\n",
        "            fname = original_fname.replace(\".txt\", f\"_{k}.txt\")\n",
        "            k += 1\n",
        "        used_names.add(fname)\n",
        "\n",
        "        # Write file (UTF-8)\n",
        "        path = os.path.join(base_dir, fname)\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "\n",
        "        saved += 1\n",
        "        if saved <= 5:  # print only the first few to keep output tidy\n",
        "            print(\"Saved:\", path)\n",
        "    print(f\"Done. Saved {saved} text files to {base_dir}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9403b38c",
      "metadata": {
        "id": "9403b38c"
      },
      "source": [
        "## 5) Ethics, Legality, and Troubleshooting\n",
        "- **robots.txt**: Check site’s crawling policy, but note it’s advisory; always follow terms of service.\n",
        "- **Rate limiting**: Sleep between requests; don’t parallelize aggressively.\n",
        "- **Attribution**: Cite sources when using scraped content in reports.\n",
        "- **Paywalls / JS‑rendered sites**: Some pages need tools like `selenium` or `requests_html`. Use sparingly and ethically.\n",
        "- **Stability**: News sites change their HTML; write resilient, minimal selectors (e.g., `find_all(\"p\")` as a start).\n",
        "- **Alternatives**: Prefer official APIs when available (structured, stable, legally safer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79bae894",
      "metadata": {
        "id": "79bae894"
      },
      "source": [
        "## 6) Mini‑Project (60 min post‑break)\n",
        "**Choose a topic** (e.g., *housing policy*, *AI regulation*, *public transit*, *Ukraine*) and:\n",
        "\n",
        "1. Modify the NewsAPI query to fetch ~25 English articles from the last week.\n",
        "2. Convert to a tidy `DataFrame`, keep: `source_name`, `title`, `description`, `url`, `publishedAt`.\n",
        "3. Scrape the first 2–3 article URLs and add a `scraped_text` column.\n",
        "4. Save your work to CSV: `results_<topic>.csv`.\n",
        "\n",
        "> **Stretch goal:** Use `.str.len()` on `scraped_text` to identify the fullest articles; compute basic stats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6962c64",
      "metadata": {
        "id": "b6962c64"
      },
      "outputs": [],
      "source": [
        "# Starter scaffold for the mini-project (students edit this cell).\n",
        "topic = \"New York\"  # ← change to your chosen topic\n",
        "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "\n",
        "news_url = \"https://newsapi.org/v2/everything\"\n",
        "params = {\n",
        "    \"q\": topic,\n",
        "    \"language\": \"en\",\n",
        "    \"pageSize\": 25,\n",
        "    \"sortBy\": \"publishedAt\",\n",
        "    \"apiKey\": NEWSAPI_KEY\n",
        "}\n",
        "resp = requests.get(news_url, params=params)\n",
        "data = resp.json()\n",
        "df_topic = pd.DataFrame(data.get(\"articles\", []))\n",
        "if not df_topic.empty:\n",
        "    df_topic[\"source_name\"] = df_topic[\"source\"].apply(lambda d: d.get(\"name\") if isinstance(d, dict) else None)\n",
        "    df_topic = df_topic.drop(columns=[\"source\"])\n",
        "    df_topic[\"publishedAt\"] = pd.to_datetime(df_topic[\"publishedAt\"], errors=\"coerce\")\n",
        "    df_topic = df_topic.dropna(subset=[\"url\", \"title\"]).sort_values(\"publishedAt\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Scrape first 3 URLs\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    texts = []\n",
        "    for i in range(min(3, len(df_topic))):\n",
        "        u = df_topic.loc[i, \"url\"]\n",
        "        try:\n",
        "            r = requests.get(u, headers=headers, timeout=15)\n",
        "            s = BeautifulSoup(r.text, \"html.parser\")\n",
        "            paras = [p.get_text(strip=True) for p in s.find_all(\"p\")]\n",
        "            texts.append(\" \".join([t for t in paras if t]))\n",
        "            time.sleep(1.0)\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "            texts.append(\"\")\n",
        "    df_topic.loc[:len(texts)-1, \"scraped_text\"] = texts\n",
        "\n",
        "    # Save to CSV\n",
        "    out_name = f\"results_{topic.replace(' ', '_').lower()}.csv\"\n",
        "    df_topic.to_csv(out_name, index=False)\n",
        "    print(\"Saved:\", out_name)\n",
        "    df_topic.head()\n",
        "else:\n",
        "    print(\"No results — check your API key or query.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765e8184",
      "metadata": {
        "id": "765e8184"
      },
      "source": [
        "## Appendix: Common Errors & Fixes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6af522f",
      "metadata": {
        "id": "c6af522f"
      },
      "outputs": [],
      "source": [
        "# 1) If you get a 401 error from NewsAPI -> invalid/expired API key.\n",
        "#    Fix: double-check your key, or set it explicitly:\n",
        "# os.environ['NEWSAPI_KEY'] = 'PASTE_YOUR_KEY_HERE'\n",
        "\n",
        "# 2) If scraping returns empty text:\n",
        "#    - Try adding headers with a real user-agent.\n",
        "#    - Try a different URL (some pages are behind paywalls or JS-rendered).\n",
        "#    - Verify that <p> tags exist by printing a snippet of soup:\n",
        "# print(soup.prettify()[:1500])\n",
        "\n",
        "# 3) If you see Unicode errors when saving CSV:\n",
        "# df.to_csv(\"file.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "# 4) If you need only recent articles, filter by date:\n",
        "# cutoff = pd.Timestamp.utcnow() - pd.Timedelta(days=7)\n",
        "# df = df[df['publishedAt'] >= cutoff]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}