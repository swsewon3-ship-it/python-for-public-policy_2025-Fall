{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swsewon3-ship-it/python-for-public-policy_2025-Fall/blob/main/Intro_Text_Analysis_TFIDF_LDA_Inaugurals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ab4d6d",
      "metadata": {
        "id": "b3ab4d6d"
      },
      "source": [
        "\n",
        "# Intro to Text Analysis in Python: FreqDist ‚Üí TF‚ÄìIDF ‚Üí Topic Modeling (U.S. Inaugural Addresses)\n",
        "\n",
        "**Course**: Intro to Text Analysis for Public Policy  \n",
        "**Format**: Live coding (~2.5 hours) + 60‚Äëmin student-driven scavenger hunt  \n",
        "**Dataset**: U.S. Presidential Inaugural Addresses (via NLTK)\n",
        "\n",
        "### Learning Outcomes\n",
        "- Load and lightly clean a real-world corpus.\n",
        "- Contrast **raw frequency (FreqDist)** vs **TF‚ÄìIDF** to understand term salience.\n",
        "- Use **topic modeling (LDA, scikit‚Äëlearn)** to uncover corpus‚Äëlevel themes.\n",
        "- Compare and interpret outputs to make policy‚Äërelevant claims.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Comparing TF‚ÄìIDF vs Topic Modeling in Policy Contexts\n",
        "\n",
        "| Policy Context | What TF‚ÄìIDF Reveals | What Topic Modeling Reveals | Example Insight |\n",
        "|----------------|--------------------|-----------------------------|-----------------|\n",
        "| üèõ **Legislative & Political Communication** | Distinctive vocabulary by legislator or party (e.g., what makes one member‚Äôs rhetoric unique) | Shared themes or issue clusters across speeches (e.g., ‚Äúhealthcare,‚Äù ‚Äúsecurity,‚Äù ‚Äúimmigration‚Äù) | TF‚ÄìIDF shows that one senator emphasizes ‚Äúopioids‚Äù while another uses ‚Äúcybersecurity‚Äù; LDA groups all health-related terms into a ‚Äúpublic health‚Äù topic. |\n",
        "| üåê **Diplomatic & Multilateral Statements** | Country-specific framing of an issue (what each nation stresses) | Global discourse patterns and alliances (how nations group around themes) | TF‚ÄìIDF highlights Fiji‚Äôs use of ‚Äúloss and damage‚Äù vs. the U.S.‚Äôs ‚Äúinnovation‚Äù; LDA identifies a broader ‚Äúclimate adaptation‚Äù topic uniting small island states. |\n",
        "| üïä **NGO & Think-Tank Reports** | Organization-specific keywords that signal focus or mandate | Latent themes that span organizations (e.g., ‚Äúeducation policy,‚Äù ‚Äúmacroeconomic reform‚Äù) | TF‚ÄìIDF shows UNICEF‚Äôs ‚Äúchild rights‚Äù language; LDA uncovers cross-agency topics like ‚Äúfinancing for development.‚Äù |\n",
        "| üì∞ **Media Coverage of Global Policy** | Outlet-specific framing and language choices | Dominant topics in media discourse across sources or time | TF‚ÄìIDF shows Fox News emphasizes ‚Äúenergy independence,‚Äù The Guardian ‚Äúclimate justice‚Äù; LDA extracts topics like ‚Äúenergy transition,‚Äù ‚Äúpolicy negotiations.‚Äù |\n",
        "| ‚öñÔ∏è **Comparative Policy Texts / Legislation** | Unique legal or regulatory phrasing in each country | Shared or evolving legal concepts across multiple texts | TF‚ÄìIDF finds Germany stresses ‚ÄúEnergiewende‚Äù; LDA surfaces a ‚Äúrenewable energy transition‚Äù topic appearing in multiple EU laws. |\n",
        "| üí¨ **Public Consultation & Citizen Feedback** | Stakeholder-specific concerns or jargon (e.g., NGOs vs. corporations) | Major themes emerging from thousands of comments | TF‚ÄìIDF identifies NGOs‚Äô use of ‚Äúpollution control‚Äù vs. industry‚Äôs ‚Äúinnovation cost‚Äù; LDA clusters all responses into ‚Äúeconomic impact,‚Äù ‚Äúenvironmental justice,‚Äù etc. |\n",
        "| üß≠ **Speeches & Strategic Messaging Over Time** | New or distinctive terms introduced in a given year or presidency | Long-term thematic evolution or cycles in national rhetoric | TF‚ÄìIDF shows ‚Äúpandemic‚Äù spikes in 2020; LDA reveals enduring topics like ‚Äúforeign policy,‚Äù ‚Äúdomestic economy,‚Äù ‚Äúnational security.‚Äù |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "| Technique | Best For | Analytical Focus |\n",
        "|------------|-----------|------------------|\n",
        "| **TF‚ÄìIDF** | Comparing documents or actors | *‚ÄúWhat makes this text distinct?‚Äù* |\n",
        "| **Topic Modeling (LDA)** | Discovering cross-document themes | *‚ÄúWhat themes recur across the corpus?‚Äù* |\n",
        "\n",
        "> ‚úÖ Together, they bridge **micro-level distinctiveness** (TF‚ÄìIDF) and **macro-level patterns** (LDA) ‚Äî enabling richer analysis of language in policy and diplomacy.\n"
      ],
      "metadata": {
        "id": "nGN0pOUJFSji"
      },
      "id": "nGN0pOUJFSji"
    },
    {
      "cell_type": "markdown",
      "id": "f5425274",
      "metadata": {
        "id": "f5425274"
      },
      "source": [
        "\n",
        "## 1) Environment Setup (Colab‚Äëfriendly)\n",
        "Run this once in Colab to install/upgrade packages and download NLTK data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3db1f1",
      "metadata": {
        "id": "cc3db1f1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# In a fresh runtime (Runtime ‚Üí Restart runtime), run:\n",
        "!pip -q install \"numpy==2.0.2\" \"scipy==1.14.1\" \"scikit-learn>=1.4\"\n",
        "!pip install nltk==3.9.2\n",
        "\n",
        "import numpy, scipy, sklearn\n",
        "print(\"NumPy:\", numpy.__version__)     # ‚Üí 2.0.2\n",
        "print(\"SciPy:\", scipy.__version__)     # ‚Üí 1.14.x\n",
        "print(\"sklearn:\", sklearn.__version__) # ‚â• 1.4\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('inaugural')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Some environments expect punkt_tab as well:\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"‚úÖ Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba91aac",
      "metadata": {
        "id": "6ba91aac"
      },
      "source": [
        "\n",
        "## 2) Imports\n",
        "We use: `nltk` for data & preprocessing, `scikit-learn` for TF‚ÄìIDF and LDA, `matplotlib/pandas` for exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8b1197",
      "metadata": {
        "id": "7c8b1197"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import inaugural, stopwords\n",
        "from nltk import word_tokenize, FreqDist\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
        "\n",
        "\n",
        "# Display all rows and columns (adjust numbers as needed)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Show full text in each cell (no truncation)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Expand the display width so wide tables don't wrap\n",
        "pd.set_option('display.width', 0)\n",
        "\n",
        "print(\"‚úÖ Pandas display options set for full view.\")\n",
        "\n",
        "print(\"‚úÖ Imports loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c2d5983",
      "metadata": {
        "id": "8c2d5983"
      },
      "source": [
        "\n",
        "## 3) Load the U.S. Presidential Inaugural Addresses\n",
        "We‚Äôll load speeches from NLTK‚Äôs `inaugural` corpus. Each document is a speech like `1789-Washington.txt`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5764a559",
      "metadata": {
        "id": "5764a559"
      },
      "outputs": [],
      "source": [
        "\n",
        "fileids = inaugural.fileids()\n",
        "print(fileids)\n",
        "\n",
        "records = []\n",
        "for fid in fileids:\n",
        "    raw = inaugural.raw(fid) #The .raw() method returns the entire text of one file as a single string ‚Äî no tokenization, no cleaning, just raw text\n",
        "    year, president = fid.replace('.txt', '').split('-')[0], fid.replace('.txt', '').split('-')[1] #extracts the year and president‚Äôs name from each file‚Äôs name in the NLTK inaugural corpus\n",
        "    records.append({'fileid': fid, 'year': int(year), 'president': president, 'text': raw})\n",
        "\n",
        "df = pd.DataFrame(records).sort_values('year').reset_index(drop=True)\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4935c79c",
      "metadata": {
        "id": "4935c79c"
      },
      "source": [
        "\n",
        "## 4) Light Preprocessing\n",
        "Simple, transparent steps:\n",
        "- lowercase ‚Üí tokenize ‚Üí keep alphabetic tokens (len ‚â• 3) ‚Üí remove stopwords  \n",
        "We also add a small custom stoplist of political boilerplate words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c29c23",
      "metadata": {
        "id": "41c29c23"
      },
      "outputs": [],
      "source": [
        "\n",
        "EN_STOP = set(stopwords.words('english'))\n",
        "print(\"English stopwords:\", EN_STOP)\n",
        "CUSTOM_STOP = {\n",
        "    # corpus artifacts / very generic political words (tweak in class)\n",
        "    'applause', 'cheers', 'government', 'nation', 'people', 'states', 'united', 'american', 'america'\n",
        "}\n",
        "STOPWORDS = EN_STOP.union(CUSTOM_STOP)\n",
        "\n",
        "def simple_clean_tokens(text):\n",
        "    \"\"\"\n",
        "    1) Lowercase\n",
        "    2) Tokenize\n",
        "    3) Keep alphabetic tokens of length >= 3\n",
        "    4) Remove stopwords\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    clean = [tok for tok in tokens if tok.isalpha() and len(tok) >= 3 and tok not in STOPWORDS]\n",
        "    return clean\n",
        "\n",
        "df['tokens'] = df['text'].apply(simple_clean_tokens)\n",
        "df['text_clean'] = df['tokens'].apply(lambda toks: \" \".join(toks))\n",
        "\n",
        "print(\"Sample tokens:\", df.loc[0, 'tokens'][:25])\n",
        "df[['fileid','year','president','text_clean']].head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ce0f4e",
      "metadata": {
        "id": "a1ce0f4e"
      },
      "source": [
        "\n",
        "## 5) Quick Exploration\n",
        "A glance at token counts and frequent words gives intuition before modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5a2577",
      "metadata": {
        "id": "ec5a2577"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Document lengths\n",
        "df['n_tokens'] = df['tokens'].apply(len)\n",
        "ax = df.plot(x='year', y='n_tokens', kind='bar', figsize=(12,4), legend=False)\n",
        "ax.set_ylabel(\"Tokens per speech (after cleaning)\")\n",
        "ax.set_xlabel(\"Index (chronological order)\")\n",
        "ax.set_title(\"Document lengths\")\n",
        "plt.show()\n",
        "\n",
        "# Global top terms (sanity check)\n",
        "all_terms = [t for toks in df['tokens'] for t in toks]\n",
        "top20 = Counter(all_terms).most_common(20)\n",
        "pd.DataFrame(top20, columns=['term','count'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a594c265",
      "metadata": {
        "id": "a594c265"
      },
      "source": [
        "\n",
        "## 5.5) Keyword Frequency (NLTK `FreqDist`) ‚Üí Why TF‚ÄìIDF?\n",
        "`FreqDist` counts words across the **entire corpus**. High counts may reflect words that are common everywhere‚Äînot necessarily distinctive.\n",
        "\n",
        "**Idea:** Use FreqDist to see the *loudest* words, then use TF‚ÄìIDF to see the *most distinctive per document*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e415a1",
      "metadata": {
        "id": "22e415a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "all_tokens = [t for toks in df['tokens'] for t in toks]\n",
        "fdist = FreqDist(all_tokens)\n",
        "\n",
        "print(\"Top 20 most frequent words across all speeches:\\n\")\n",
        "for word, freq in fdist.most_common(20):\n",
        "    print(f\"{word:15s} {freq}\")\n",
        "\n",
        "# Visualize (optional)\n",
        "plt.figure(figsize=(10,4))\n",
        "fdist.plot(20, cumulative=False)\n",
        "plt.title(\"Most Frequent Words in Inaugural Speeches (Cleaned)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a62ec2",
      "metadata": {
        "id": "60a62ec2"
      },
      "source": [
        "\n",
        "## 6) TF‚ÄìIDF with scikit‚Äëlearn\n",
        "**TF‚ÄìIDF** = term frequency √ó inverse document frequency  \n",
        "- Highlights terms that are frequent **and** specific to a document.  \n",
        "- Downweights terms that appear in many documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d57b134",
      "metadata": {
        "id": "8d57b134"
      },
      "outputs": [],
      "source": [
        "\n",
        "# convert a collection of text documents (your speeches) into a matrix where: Each row = one document (a speech);\n",
        "# Each column = one term (a word);\n",
        "# Each cell value = TF‚ÄìIDF weight of that term in that document\n",
        "# min_df=2 ‚Üí ignore words that appear in fewer than 2 documents\n",
        "tfidf = TfidfVectorizer(min_df=2)\n",
        "\n",
        "# Feeds your cleaned text (from the text_clean column) into the vectorizer.\n",
        "# Two steps happen in one command:\n",
        "# .fit() ‚Äî learns the vocabulary and IDF (Inverse Document Frequency) weights.\n",
        "# .transform() ‚Äî applies the TF‚ÄìIDF transformation to each document.\n",
        "# Returns a sparse matrix X_tfidf of shape:\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(df['text_clean'])\n",
        "\n",
        "# Retrieves the list of all terms (vocabulary) that the vectorizer kept.\n",
        "# Converts it into a NumPy array for easy indexing and sorting later.\n",
        "# You‚Äôll use it when finding the top TF‚ÄìIDF terms for each speech:\n",
        "\n",
        "terms = np.array(tfidf.get_feature_names_out())\n",
        "\n",
        "# ((# of docs, # of unique words), # of unique words)\n",
        "X_tfidf.shape, len(terms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf862e8f",
      "metadata": {
        "id": "cf862e8f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def top_tfidf_terms_for_doc(doc_idx, top_n=12): #Define a function that returns the top-n TF‚ÄìIDF terms for a single document (speech).\n",
        "    row = X_tfidf.getrow(doc_idx).toarray().ravel() #row is a vector of TF‚ÄìIDF scores for one speech, where each position corresponds to one word in terms\n",
        "    top_idx = row.argsort()[::-1][:top_n] #top_idx = positions of the most distinctive words in this speech.\n",
        "    # terms[top_idx] gets the actual word strings for those indices.\n",
        "    # row[top_idx] gets their corresponding TF‚ÄìIDF scores.\n",
        "    # zip(...) pairs each word with its score.\n",
        "    # list(...) turns that into a list of (word, score) tuples.\n",
        "    return list(zip(terms[top_idx], row[top_idx]))\n",
        "\n",
        "# Show a few speeches (early, middle, recent)\n",
        "# This loop picks three speeches: The first (i = 0), The middle one (len(df)//2), The last one (len(df)-1)\n",
        "for i in [0, len(df)//2, len(df)-1]:\n",
        "    print(f\"\\n=== {df.loc[i, 'year']} - {df.loc[i, 'president']} ===\") #Prints a header showing which speech you‚Äôre examining\n",
        "    for term, score in top_tfidf_terms_for_doc(i, top_n=12): #Calls the function to get the top 12 terms for that document\n",
        "        print(f\"{term:15s} {score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad59b25",
      "metadata": {
        "id": "9ad59b25"
      },
      "source": [
        "\n",
        "### TF‚ÄìIDF Similarity (Cosine)\n",
        "Find which speeches are lexically similar using cosine similarity on TF‚ÄìIDF vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e738e517",
      "metadata": {
        "id": "e738e517"
      },
      "outputs": [],
      "source": [
        "\n",
        "sim = cosine_similarity(X_tfidf) #Computes a cosine similarity matrix for all speeches\n",
        "target = len(df) - 1  # Chooses the most recent speech (the last row in your DataFrame) as the target document.\n",
        "pairs = [(i, sim[target, i]) for i in range(len(df)) if i != target] #Builds a list of tuples for every other speech, skips the target one\n",
        "pairs_sorted = sorted(pairs, key=lambda x: x[1], reverse=True)[:5] #Sorts the list of (index, similarity) pairs by similarity score in descending order\n",
        "\n",
        "print(f\"Most similar to {df.loc[target,'year']} - {df.loc[target,'president']}:\")\n",
        "for idx, score in pairs_sorted: #Iterates over the top-5 most similar speeches\n",
        "    print(f\"  {df.loc[idx,'year']} - {df.loc[idx,'president']:12s}  (cosine={score:.3f})\")\n",
        "\n",
        "# Cosine similarity treats each speech as a high-dimensional vector (words as axes).\n",
        "# The closer the angle between two vectors, the more similar their language use ‚Äî even if the speeches differ in length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029d946e",
      "metadata": {
        "id": "029d946e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2D projection (small corpus ‚Üí OK to densify)\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "coords = pca.fit_transform(X_tfidf.toarray())\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "ax.scatter(coords[:,0], coords[:,1])\n",
        "for i, row in df.iterrows():\n",
        "    ax.annotate(str(row['year']), (coords[i,0], coords[i,1]), fontsize=8)\n",
        "ax.set_title(\"Speeches in TF‚ÄìIDF space (PCA projection)\")\n",
        "ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üé® How to interpret the chart\n",
        "1. Each point = a speech represented by its overall word usage pattern\n",
        "\n",
        "* Two speeches close together ‚Üí use similar vocabularies (in terms of TF‚ÄìIDF weighting).\n",
        "\n",
        "* Far apart ‚Üí distinct word usage ‚Äî different priorities, tone, or historical context.\n",
        "\n",
        "2. Axes (PC1 and PC2) are abstract ‚Äî they don‚Äôt correspond to literal variables\n",
        "\n",
        "* PCA components are linear combinations of all TF‚ÄìIDF features (words).\n",
        "\n",
        "* You can‚Äôt say ‚Äúthe x-axis means optimism vs war,‚Äù but you can say:\n",
        "\n",
        "* ‚ÄúAlong PC1, speeches separate based on major vocabulary differences ‚Äî early vs modern language, perhaps.‚Äù\n",
        "\n",
        "* You can interpret them qualitatively by checking which speeches cluster together.\n",
        "\n",
        "3. Look for clusters, trends, or outliers\n",
        "\n",
        "* Clusters of speeches by nearby years ‚Üí continuity in rhetoric or themes.\n",
        "\n",
        "* Isolated points ‚Üí outlier speeches (perhaps unusually short, poetic, or issue-focused).\n",
        "\n",
        "* You might see a chronological gradient: early 1800s on one side, 2000s on another ‚Äî showing the evolution of presidential language."
      ],
      "metadata": {
        "id": "B79yuxjjC0DB"
      },
      "id": "B79yuxjjC0DB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Concept   | Interpretation                                           |\n",
        "| --------- | -------------------------------------------------------- |\n",
        "| Distance  | How different two speeches‚Äô vocabularies are             |\n",
        "| Clusters  | Shared themes, era, or rhetorical style                  |\n",
        "| Outliers  | Unique speeches that break linguistic patterns           |\n",
        "| PC1 / PC2 | Major axes of variation in word use ‚Äî not literal topics |\n"
      ],
      "metadata": {
        "id": "0GMR8NyTDhgi"
      },
      "id": "0GMR8NyTDhgi"
    },
    {
      "cell_type": "code",
      "source": [
        "df[['year','president']].assign(PC1=coords[:,0], PC2=coords[:,1]).sort_values('PC1').head()\n"
      ],
      "metadata": {
        "id": "jTYEiFYRDKMr"
      },
      "id": "jTYEiFYRDKMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üé® Visualizing TF‚ÄìIDF: Word Cloud & Temporal Trend\n",
        "\n",
        "Now that we‚Äôve mapped speeches in abstract ‚ÄúTF‚ÄìIDF space,‚Äù  \n",
        "let‚Äôs explore two other ways to *see* what TF‚ÄìIDF tells us.\n",
        "\n",
        "1. **Word Cloud** ‚Äì visually emphasizes the distinctive words in one speech.  \n",
        "   - Larger words = higher TF‚ÄìIDF scores.  \n",
        "   - Great for quick, qualitative insight into what stands out.\n",
        "\n",
        "2. **Temporal Line Chart** ‚Äì tracks how the importance of a given term changes over time.  \n",
        "   - Example: does *‚Äúfreedom‚Äù* rise or fall in salience across U.S. history?\n"
      ],
      "metadata": {
        "id": "B-Szy_gMGd7L"
      },
      "id": "B-Szy_gMGd7L"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1) Word Cloud for a Selected Speech ---\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Pick a speech by index (0=earliest, -1=latest)\n",
        "doc_idx = len(df) - 1  # last speech by default\n",
        "\n",
        "# Generate dictionary of top TF‚ÄìIDF terms\n",
        "wc_data = dict(top_tfidf_terms_for_doc(doc_idx, top_n=100))\n",
        "\n",
        "# Create and display the word cloud\n",
        "wc = WordCloud(width=800, height=400, background_color='white')\n",
        "wc.generate_from_frequencies(wc_data)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(f\"Word Cloud: {df.loc[doc_idx,'year']} ‚Äì {df.loc[doc_idx,'president']}\", fontsize=14)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "mQf5AvbuGevM"
      },
      "id": "mQf5AvbuGevM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2) Temporal Line Chart of a Word's TF‚ÄìIDF Weight ---\n",
        "# Choose a term to track over time\n",
        "term = \"freedom\"  # try swapping to 'war', 'peace', 'america', etc.\n",
        "\n",
        "if term in terms:\n",
        "    term_idx = np.where(terms == term)[0][0]\n",
        "    df[f\"tfidf_{term}\"] = X_tfidf[:, term_idx].toarray().ravel()\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(df['year'], df[f\"tfidf_{term}\"], marker='o', linestyle='-')\n",
        "    plt.title(f'TF‚ÄìIDF Weight of \"{term}\" Over Time', fontsize=14)\n",
        "    plt.xlabel(\"Year of Inaugural Address\")\n",
        "    plt.ylabel(\"TF‚ÄìIDF Score\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f'Term \"{term}\" not found in vocabulary. Try another word.')"
      ],
      "metadata": {
        "id": "x0DOeBeqGpIF"
      },
      "id": "x0DOeBeqGpIF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "26337e7d",
      "metadata": {
        "id": "26337e7d"
      },
      "source": [
        "\n",
        "> ‚úÖ **What TF‚ÄìIDF tells us**: which words uniquely characterize each speech; which speeches use similar vocabularies.  \n",
        "> ‚ùå **What it doesn‚Äôt**: explicitly uncover *themes* shared across documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed817474",
      "metadata": {
        "id": "ed817474"
      },
      "source": [
        "\n",
        "## 7) Topic Modeling with scikit‚Äëlearn‚Äôs LDA\n",
        "**Latent Dirichlet Allocation (LDA)** models each document as a mixture of **topics** (word distributions).  \n",
        "We‚Äôll build a bag‚Äëof‚Äëwords matrix, fit an LDA model, inspect topics, and examine per‚Äëspeech topic mixtures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a72067e5",
      "metadata": {
        "id": "a72067e5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Bag-of-words for LDA\n",
        "  # CountVectorizer converts each document into a bag-of-words (word counts, not weights).\n",
        "  # min_df=2 means: ignore words that appear in fewer than 2 speeches (to reduce noise).\n",
        "  # fit_transform() builds the vocabulary and creates a document-term matrix: Rows = speeches; Columns = unique words; Values = how many times each word appears\n",
        "  # vocab holds the list of all words (for displaying topic terms later).\n",
        "\n",
        "cv = CountVectorizer(min_df=2)\n",
        "X_counts = cv.fit_transform(df['text_clean'])\n",
        "vocab = np.array(cv.get_feature_names_out())\n",
        "\n",
        "# Train LDA\n",
        "  # K is the number of topics you want the model to find.\n",
        "  # This is not learned automatically ‚Äî it‚Äôs a parameter you choose.\n",
        "  # Try experimenting with different values:\n",
        "    # K=5 ‚Üí broader, more general themes (e.g., ‚Äúwar,‚Äù ‚Äúeconomy,‚Äù ‚Äúunity‚Äù).\n",
        "    # K=8 ‚Üí more nuanced topics (e.g., ‚Äúforeign policy,‚Äù ‚Äúdomestic economy,‚Äù ‚Äúliberty‚Äù).\n",
        "\n",
        "K = 8  # adjust live (5, 8, 12)\n",
        "\n",
        "\n",
        "# Initializes the LDA model from scikit-learn.\n",
        "  # n_components=K ‚Üí tells the model how many topics (components) to find.\n",
        "  # learning_method=\"batch\" ‚Üí trains on the entire dataset at once.\n",
        "  # (Alternative: \"online\" trains incrementally on chunks; ‚Äúbatch‚Äù is stable for small corpora like this.)\n",
        "  # random_state=42 ‚Üí ensures reproducible results (so every student gets the same topics).\n",
        "  # max_iter=20 ‚Üí number of passes over the data to improve the model; more iterations = more refined topics, but slower training.\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=K,\n",
        "    learning_method=\"batch\",\n",
        "    random_state=42,\n",
        "    max_iter=20\n",
        ")\n",
        "\n",
        "# This line fits the model and simultaneously transforms the data into topic proportions\n",
        "topic_mix = lda.fit_transform(X_counts)  # theta: (n_docs, K)\n",
        "\n",
        "def show_topics(model, vocab, topn=12):\n",
        "# Defines a helper function called show_topics().\n",
        "# Inputs:\n",
        "  # model ‚Üí your trained LDA model (lda).\n",
        "  # vocab ‚Üí array of all words in your vocabulary (from CountVectorizer).\n",
        "  # topn ‚Üí how many top words you want to display per topic (default = 12).\n",
        "    for k, comp in enumerate(model.components_):\n",
        "    # model.components_ is a 2D NumPy array where:\n",
        "    # Each row corresponds to a topic (Topic 0, Topic 1, ‚Ä¶).\n",
        "    # Each column corresponds to a word in the vocabulary.\n",
        "    # Each value = the importance (weight) of that word within the topic.\n",
        "    # enumerate() loops through all topics (k) and their corresponding word-weight vectors (comp).\n",
        "        top_idx = comp.argsort()[::-1][:topn]\n",
        "        # argsort() returns the indices that would sort the array ‚Äî here, the word weights ‚Äî in ascending order.\n",
        "        # [::-1] reverses that order to descending (highest-weighted words first).\n",
        "        # [:topn] takes only the top n indices (e.g., top 12 words).\n",
        "        print(f\"\\nTopic {k}: \" + \", \".join(vocab[top_idx]))\n",
        "\n",
        "show_topics(lda, vocab, topn=12)\n",
        "\n",
        "# Assemble per-document topic proportions\n",
        "topic_df = pd.DataFrame(topic_mix, columns=[f\"topic_{k}\" for k in range(K)])\n",
        "result_df = pd.concat([df[['fileid','year','president']], topic_df], axis=1)\n",
        "result_df.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map the topics across a heatmap\n",
        "\n",
        "üß† How to Interpret the LDA Topic Heatmap\n",
        "\n",
        "Each cell of the heatmap represents the proportion of a given topic within a specific speech.\n",
        "Color intensity encodes how strongly that topic appears ‚Äî darker (or brighter) = higher proportion, lighter = weaker presence."
      ],
      "metadata": {
        "id": "dE3ZhwPzMbOd"
      },
      "id": "dE3ZhwPzMbOd"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install plotly\n",
        "\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n"
      ],
      "metadata": {
        "id": "lquemKUhLAvS"
      },
      "id": "lquemKUhLAvS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topic_top_words(lda_model, vocab, topn=10):\n",
        "    \"\"\"Return:\n",
        "       - topic_labels: list like [\"T0: economy, growth, jobs\", ...]\n",
        "       - topic_words:  list of lists of the topn words per topic (for hover)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    words_list = []\n",
        "    for k, comp in enumerate(lda_model.components_):\n",
        "        top_idx = comp.argsort()[::-1][:topn]\n",
        "        words = vocab[top_idx].tolist()\n",
        "        words_list.append(words)\n",
        "        label = f\"T{k}: \" + \", \".join(words[:6])  # concise label for axis/hover\n",
        "        labels.append(label)\n",
        "    return labels, words_list\n",
        "\n",
        "topic_labels, topic_words = topic_top_words(lda, vocab, topn=12)\n",
        "\n",
        "# Columns in result_df that are topic proportions\n",
        "topic_cols = [c for c in result_df.columns if c.startswith(\"topic_\")]\n"
      ],
      "metadata": {
        "id": "GvfygMktLBXM"
      },
      "id": "GvfygMktLBXM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick rows you want to compare\n",
        "rows = [0, len(result_df)//2, len(result_df)-1] #The list [0, len(result_df)//2, len(result_df)-1] = [first_speech, middle_speech, last_speech]\n",
        "df_sel = result_df.iloc[rows].copy()\n",
        "\n",
        "# This code converts the selected speeches‚Äô topic proportions into a NumPy matrix (Z) for plotting,\n",
        "# creates x-axis labels (x) showing topic numbers (like ‚ÄúT0‚Äù, ‚ÄúT1‚Äù, ‚Ä¶), and builds y-axis labels (y)\n",
        "# combining each speech‚Äôs year and president name for the heatmap\n",
        "Z = df_sel[topic_cols].to_numpy()\n",
        "x = [f\"T{int(c.split('_')[-1])}\" for c in topic_cols]\n",
        "y = [f\"{r.year} ‚Äî {r.president}\" for _, r in df_sel.iterrows()]\n",
        "\n",
        "# Build hovertext matrix: one string per cell\n",
        "hovertext = []\n",
        "for r_i, r in df_sel.iterrows():\n",
        "    row_texts = []\n",
        "    for t_i, col in enumerate(topic_cols):\n",
        "        k = int(col.split('_')[-1])\n",
        "        row_texts.append(\n",
        "            f\"<b>{int(r.year)} ‚Äî {r.president}</b><br>\"\n",
        "            f\"<b>Topic {k}</b><br>\"\n",
        "            f\"Top words: {', '.join(topic_words[k][:10])}<br>\"\n",
        "            f\"Proportion: {r[col]:.3f}\"\n",
        "        )\n",
        "    hovertext.append(row_texts)\n",
        "\n",
        "fig = go.Figure(\n",
        "    data=go.Heatmap(\n",
        "        z=Z,\n",
        "        x=x,\n",
        "        y=y,\n",
        "        colorscale=\"Plasma\",\n",
        "        zmin=0.0, zmax=1.0,\n",
        "        hoverinfo=\"text\",\n",
        "        text=hovertext\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Topic mixture (theta) ‚Äî selected speeches\",\n",
        "    xaxis_title=\"Topic\",\n",
        "    yaxis_title=\"Speech\",\n",
        "    height=300 + 40*len(rows),\n",
        "    margin=dict(l=80, r=20, t=60, b=60)\n",
        ")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "88BmoX9oLD9S"
      },
      "id": "88BmoX9oLD9S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure chronological order\n",
        "df_sorted = result_df.sort_values(\"year\").reset_index(drop=True)\n",
        "\n",
        "# Reorder topics by global prevalence (more interpretable)\n",
        "mean_by_topic = df_sorted[topic_cols].mean(axis=0).to_numpy()\n",
        "order = np.argsort(mean_by_topic)[::-1]\n",
        "ordered_cols = [topic_cols[i] for i in order]\n",
        "ordered_x = [f\"T{int(c.split('_')[-1])}\" for c in ordered_cols]\n",
        "\n",
        "A = df_sorted[ordered_cols].to_numpy()\n",
        "y_all = df_sorted[\"year\"].astype(str) + \" ‚Äî \" + df_sorted[\"president\"]\n",
        "\n",
        "# Hovertext matrix for all speeches\n",
        "hovertext_all = []\n",
        "for r_i, r in df_sorted.iterrows():\n",
        "    row_texts = []\n",
        "    for c in ordered_cols:\n",
        "        k = int(c.split('_')[-1])\n",
        "        row_texts.append(\n",
        "            f\"<b>{int(r['year'])} ‚Äî {r['president']}</b><br>\"\n",
        "            f\"<b>Topic {k}</b><br>\"\n",
        "            f\"Top words: {', '.join(topic_words[k][:10])}<br>\"\n",
        "            f\"Proportion: {r[c]:.3f}\"\n",
        "        )\n",
        "    hovertext_all.append(row_texts)\n",
        "\n",
        "fig_all = go.Figure(\n",
        "    data=go.Heatmap(\n",
        "        z=A,\n",
        "        x=ordered_x,\n",
        "        y=y_all,\n",
        "        colorscale=\"Plasma\",\n",
        "        zmin=0.0, zmax=1.0,\n",
        "        hoverinfo=\"text\",\n",
        "        text=hovertext_all\n",
        "    )\n",
        ")\n",
        "fig_all.update_layout(\n",
        "    title=\"All speeches ‚Äî topic mixture heatmap (topics ordered by prevalence)\",\n",
        "    xaxis_title=\"Topic\",\n",
        "    yaxis_title=\"Speech (year ‚Äî president)\",\n",
        "    height=max(450, 14*len(df_sorted)),\n",
        "    margin=dict(l=120, r=20, t=60, b=80)\n",
        ")\n",
        "fig_all.show()\n"
      ],
      "metadata": {
        "id": "kgyApwStLIJa"
      },
      "id": "kgyApwStLIJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9f0740f6",
      "metadata": {
        "id": "9f0740f6"
      },
      "source": [
        "\n",
        "## 8) TF‚ÄìIDF vs LDA ‚Äî Compare & Contrast\n",
        "| Aspect | TF‚ÄìIDF | LDA (Topics) |\n",
        "|---|---|---|\n",
        "| Unit | Terms per document | Topics (word dists); documents are mixtures |\n",
        "| Great for | Keywording, distinctiveness, similarity | Thematic mapping across corpus |\n",
        "| Limitations | No explicit themes | Needs K tuning; topics can blend/split |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db325d75",
      "metadata": {
        "id": "db325d75"
      },
      "source": [
        "## 9) üéØ Student-Driven Policy Exploration (‚âà60 minutes)\n",
        "\n",
        "Work in pairs. Your mission: **choose a policy area**, **build a small text corpus**, and **experiment** with TF‚ÄìIDF and topic modeling to discover what language patterns define that space.\n",
        "\n",
        "This is not a graded deliverable ‚Äî it‚Äôs a sandbox for exploration, pattern-finding, and discussion.\n",
        "\n",
        "---\n",
        "\n",
        "### üß≠ Part A ‚Äî Choose a Policy Area\n",
        "Pick an issue you care about ‚Äî examples:\n",
        "\n",
        "- Climate policy & sustainability  \n",
        "- Immigration & border security  \n",
        "- Health care & public health  \n",
        "- Economic growth & inequality  \n",
        "- Civil rights & social justice  \n",
        "- Foreign policy & diplomacy  \n",
        "\n",
        "Then brainstorm: *Whose language represents this issue?*  \n",
        "(e.g., presidents, UN leaders, legislators, NGOs, media outlets).\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Part B ‚Äî Build Your Corpus\n",
        "\n",
        "You‚Äôll need at least **10‚Äì20 short to medium speeches or statements**.\n",
        "\n",
        "**Option 1 ‚Äì Use existing open archives**\n",
        "- U.S. presidential speeches: [American Presidency Project](https://www.presidency.ucsb.edu/speeches)\n",
        "- UN General Assembly statements: [UN Digital Library](https://digitallibrary.un.org/)\n",
        "- EU or UK parliament debates: [Hansard](https://hansard.parliament.uk/), [Europarl](https://www.europarl.europa.eu/)\n",
        "- NGO or think-tank reports: World Bank, IMF, WHO, Brookings, RAND, etc.\n",
        "\n",
        "**Option 2 ‚Äì Scrape or collect your own (advanced)**\n",
        "- Use `requests` + `BeautifulSoup` or a library such as `newspaper3k` to extract text.\n",
        "- Or copy/paste short excerpts into `.txt` files and upload them to Colab.\n",
        "\n",
        "üìé *Hint:* keep your text clean ‚Äî remove headers, speaker names, and references.\n",
        "\n",
        "See script below to get you started\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Part C ‚Äî Explore Frequency, TF‚ÄìIDF, and Topics\n",
        "\n",
        "1. **Frequency snapshot:**  \n",
        "   Compute the top 15 most common words (`FreqDist`). Which ones are generic boilerplate (e.g., ‚Äúpeople,‚Äù ‚Äúgovernment‚Äù)?\n",
        "\n",
        "2. **Distinctiveness check:**  \n",
        "   Run **TF‚ÄìIDF** with `min_df=2` or `min_df=5`.  \n",
        "   - Which words rise to the top?  \n",
        "   - What do they reveal about your policy domain‚Äôs unique framing?\n",
        "\n",
        "3. **Similarity sleuthing:**  \n",
        "   Using cosine similarity on TF‚ÄìIDF vectors, find which two documents are most similar.  \n",
        "   What links them ‚Äî era, country, tone?\n",
        "\n",
        "4. **Topic discovery:**  \n",
        "   Train an **LDA model** (try `K=5`, `K=8`, `K=12`).  \n",
        "   - Label each topic in 2‚Äì3 words.  \n",
        "   - Which `K` feels most interpretable?  \n",
        "   - Do your topics align with known sub-issues (e.g., ‚Äúenergy transition,‚Äù ‚Äúhuman rights,‚Äù ‚Äútrade policy‚Äù)?\n",
        "\n",
        "5. **Visualize:**  \n",
        "   Create a PCA or heatmap of your documents.  \n",
        "   - What clusters appear?  \n",
        "   - Does time, geography, or institution explain them?\n",
        "\n",
        "---\n",
        "\n",
        "### üïµÔ∏è Part D ‚Äî Mini Scavenger Hunt Prompts\n",
        "\n",
        "- **‚ÄúWord Detective‚Äù**: Which words define your corpus when using TF‚ÄìIDF vs raw frequency?  \n",
        "- **‚ÄúSimilarity Sleuth‚Äù**: Which two documents look similar numerically but differ substantively?  \n",
        "- **‚ÄúTopic Whisperer‚Äù**: Choose one topic from your LDA output. Find two speeches that heavily feature it (> 0.3). What do they share?  \n",
        "- **‚ÄúEra Shift‚Äù**: Does any topic fade or grow over time? What might explain it?  \n",
        "- **‚ÄúHeadline Writer‚Äù**: Summarize one document twice ‚Äî once using TF‚ÄìIDF terms, once using its dominant LDA topic. How do the headlines differ in tone?\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Part E ‚Äî Policy Reflection (Discussion, not submission)\n",
        "\n",
        "Compare what each method tells you:\n",
        "\n",
        "| Method | Reveals | Best for |\n",
        "|---------|----------|----------|\n",
        "| **TF‚ÄìIDF** | Distinctive vocabulary per document | Comparing actors or countries |\n",
        "| **LDA (Topic Modeling)** | Underlying shared themes | Tracking issue clusters and framing evolution |\n",
        "\n",
        "> In your discussion:  \n",
        "> - What language dominates your policy area?  \n",
        "> - Whose framing or rhetoric stands out?  \n",
        "> - How might these tools support evidence-based policy analysis?\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Outcome:** You should be able to *talk through* what you learned ‚Äî\n",
        "not produce a written report. Your goal is pattern recognition, curiosity, and connecting computational text analysis to real policy discourse.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# üß≠ STEP 1: Mount Google Drive\n",
        "# =======================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YyISvb9LWnrq"
      },
      "id": "YyISvb9LWnrq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# üóÇ STEP 2: Create a folder in Google Drive for text corpus\n",
        "# =======================================================\n",
        "import os\n",
        "\n",
        "# Customize the folder name ‚Äî each student can use their initials or topic\n",
        "folder_name = \"policy_corpus\"\n",
        "drive_path = \"/content/drive/MyDrive\"\n",
        "corpus_dir = os.path.join(drive_path, folder_name)\n",
        "\n",
        "os.makedirs(corpus_dir, exist_ok=True)\n",
        "print(f\"‚úÖ Folder ready: {corpus_dir}\")\n"
      ],
      "metadata": {
        "id": "ccWc4o7WW1df"
      },
      "id": "ccWc4o7WW1df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# üì∞ STEP 3: Scrape Articles with newspaper3k and Save as .txt\n",
        "# =======================================================\n",
        "\n",
        "!pip install newspaper3k lxml_html_clean --quiet\n",
        "# !pip -q install newspaper3k lxml_html_clean\n",
        "\n",
        "# Import after successful install\n",
        "from newspaper import Article\n",
        "\n",
        "\n",
        "\n",
        "import time, os, requests\n",
        "from newspaper import Article, Config\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/123.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "\n",
        "cfg = Config()\n",
        "cfg.browser_user_agent = HEADERS[\"User-Agent\"]\n",
        "cfg.request_timeout = 20\n",
        "cfg.memoize_articles = False\n",
        "\n",
        "def extract_with_newspaper(url: str) -> str:\n",
        "    \"\"\"Try Newspaper with real UA; if download() gets 403, use requests + set_html().\"\"\"\n",
        "    art = Article(url, config=cfg)\n",
        "    try:\n",
        "        art.download()              # may 403\n",
        "        art.parse()\n",
        "        return art.text.strip()\n",
        "    except Exception:\n",
        "        # Fallback: fetch with requests using real headers, then feed raw HTML to Newspaper\n",
        "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()        # will throw if not 200 (but you said it's 200)\n",
        "        art = Article(url, config=cfg)\n",
        "        art.set_html(r.text)\n",
        "        art.parse()\n",
        "        return art.text.strip()\n",
        "\n",
        "\n",
        "\n",
        "import pathlib\n",
        "SAVE_DIR = pathlib.Path(\"/content/drive/MyDrive/policy_corpus\")\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def safe_filename(title: str, i: int) -> str:\n",
        "    base = \"\".join(c for c in title if c.isalnum() or c in (\" \",\"_\")).strip().replace(\" \",\"_\")\n",
        "    if not base: base = f\"article_{i}\"\n",
        "    return f\"{i:02d}_{base[:60]}.txt\"\n",
        "\n",
        "def extract_article(url: str, i: int) -> str:\n",
        "    # Try Newspaper (UA) ‚Üí Newspaper with requests HTML ‚Üí Trafilatura\n",
        "    try:\n",
        "        text = extract_with_newspaper(url)\n",
        "        source = \"newspaper3k\"\n",
        "    except Exception as e1:\n",
        "        try:\n",
        "            text = extract_with_trafilatura(url)\n",
        "            source = \"trafilatura\"\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(f\"Both extractors failed.\\nNewspaper err: {e1}\\nTrafilatura err: {e2}\")\n",
        "    return text, source\n",
        "\n",
        "def save_article(urls):\n",
        "    import datetime\n",
        "    for i, url in enumerate(urls, start=1):\n",
        "        try:\n",
        "            text, source = extract_article(url, i)\n",
        "            title_hint = url.split(\"/\")[-2] if \"/\" in url else \"article\"\n",
        "            fname = safe_filename(title_hint, i)\n",
        "            fpath = SAVE_DIR / fname\n",
        "            with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"URL: {url}\\n\")\n",
        "                f.write(f\"SourceExtractor: {source}\\n\")\n",
        "                f.write(f\"SavedAtUTC: {datetime.datetime.utcnow().isoformat()}Z\\n\\n\")\n",
        "                f.write(text)\n",
        "            print(f\"‚úÖ Saved ({source}): {fname}\")\n",
        "            time.sleep(1.0)  # be polite\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Skipped {url}: {e}\")\n",
        "\n",
        "# EXAMPLE URLS (swap in your policy-area links)\n",
        "urls = [\n",
        "  \"https://www.un.org/sg/en/content/sg/statements/2025-11-08/secretary-generals-message-the-20th-conference-of-youth-climate-change\",\n",
        "    \"https://www.un.org/sg/en/content/sg/statements/2025-11-07/secretary-generals-remarks-the-belem-climate-summit-energy-transition-roundtable-delivered\",\n",
        "    \"https://www.un.org/sg/en/content/sg/statements/2025-11-06/secretary-generals-remarks-the-launch-of-the-tropical-forest-forever-facility-delivered\",\n",
        "]\n",
        "save_article(urls)\n",
        "print(\"Folder:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "NImi8adbZS8l"
      },
      "id": "NImi8adbZS8l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# üßæ STEP 4: Verify Saved Files\n",
        "# =======================================================\n",
        "import glob\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(corpus_dir, \"*.txt\")))\n",
        "print(f\"Found {len(files)} text files in Drive.\")\n",
        "for f in files:\n",
        "    print(\"-\", os.path.basename(f))\n"
      ],
      "metadata": {
        "id": "yxtVA44TW7-D"
      },
      "id": "yxtVA44TW7-D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a4a54875",
      "metadata": {
        "id": "a4a54875"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Closing Thought\n",
        "**FreqDist** shows what‚Äôs loudest. **TF‚ÄìIDF** shows what‚Äôs distinctive. **LDA** shows what‚Äôs thematic. Use all three to triangulate insights for public‚Äëpolicy questions.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}